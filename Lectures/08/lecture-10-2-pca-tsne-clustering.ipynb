{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_json(\"../data/news_train.json\", orient='records')\n",
    "\n",
    "df_train.info()\n",
    "df_train.hist()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby([\"label\"]).count().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "w_n_lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(lambda row: row.lower())\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(lambda row: \" \".join([w_n_lemmatizer.lemmatize(word) for word in word_tokenize(row) if not word in stop_words and word.isalpha()]))\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text vectorizing with embedding word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "vectors = []\n",
    "\n",
    "for item in tqdm(df_train[\"text\"].values):\n",
    "    doc = nlp(item)\n",
    "    tmp = [word.vector for word in doc]\n",
    "    count = len(tmp)\n",
    "    vectors.append(sum(tmp) / count)\n",
    "\n",
    "vectors = np.array(vectors)\n",
    "\n",
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "maxK=21\n",
    "step=1\n",
    "K = range(2, maxK, step)\n",
    "\n",
    "distortions = []\n",
    "tbar = tqdm(K)\n",
    "for k in tbar:\n",
    "    kmeanModel = KMeans(n_clusters=k, verbose=0)\n",
    "    kmeanModel.fit(vectors)\n",
    "    distortions.append(sum(np.min(cdist(vectors, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / vectors.shape[0])\n",
    "    tbar.set_description(\"K: \" + str(k))\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=range(2, 20, 1), metric=\"distortion\")\n",
    "visualizer.fit(vectors)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.elbow_value_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_cluster_naumber = visualizer.elbow_value_\n",
    "clusterer = KMeans(n_clusters=optimal_cluster_naumber)\n",
    "cluster_labels = clusterer.fit_predict(vectors)\n",
    "len(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne = tsne.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"tsne-1d\"] = tsne[:,0]\n",
    "df_train[\"tsne-2d\"] = tsne[:,1]\n",
    "\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_train['cluster'] = cluster_labels\n",
    "\n",
    "sns.scatterplot(data=df_train,\n",
    "                hue=\"cluster\",\n",
    "                palette=sns.color_palette(\"hls\", optimal_cluster_naumber),\n",
    "                x=\"tsne-1d\", \n",
    "                y=\"tsne-2d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_2_result = pca_2.fit_transform(vectors)\n",
    "\n",
    "print(\"Cumulative variation for 2 principal components: \", np.sum(pca_2.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"pca-1d\"] = pca_2_result[:,0]\n",
    "df_train[\"pca-2d\"] = pca_2_result[:,1]\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_train,\n",
    "                hue=\"cluster\",\n",
    "                palette=sns.color_palette(\"hls\", optimal_cluster_naumber),\n",
    "                x=\"pca-1d\", \n",
    "                y=\"pca-2d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_evr(evr, x_min=0, x_max=800, x_step=50, y_min = 0, y_max=1., y_step=0.1):\n",
    "  fig = plt.figure()\n",
    "  ax = fig.gca()\n",
    "  ax.set_xticks(np.arange(x_min, x_max, x_step))\n",
    "  ax.set_yticks(np.arange(y_min, y_max, y_step))\n",
    "  plt.plot(evr)\n",
    "  plt.grid(linestyle='-', linewidth=1)\n",
    "\n",
    "\n",
    "  plt.xlabel('number of components')\n",
    "  plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(vectors)\n",
    "evr = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "vis_evr(evr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_evr(evr[40:100], x_max=60, x_step=5, y_step=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_n = PCA(n_components=50)\n",
    "pca_n_result = pca_n.fit_transform(vectors)\n",
    "\n",
    "print(\"Cumulative variation for 50 principal components: \", np.sum(pca_n.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_json(\"../data/news_test.json\", orient='records')\n",
    "\n",
    "df_test.info()\n",
    "df_test.hist()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"text\"] = df_test[\"text\"].apply(lambda row: row.lower())\n",
    "df_test[\"text\"] = df_test[\"text\"].apply(lambda row: \" \".join([w_n_lemmatizer.lemmatize(word) for word in word_tokenize(row) if not word in stop_words and word.isalpha()]))\n",
    "\n",
    "vectors_test = []\n",
    "\n",
    "for item in tqdm(df_test[\"text\"].values):\n",
    "    doc = nlp(item)\n",
    "    tmp = [word.vector for word in doc]\n",
    "    count = len(tmp)\n",
    "    vectors_test.append(sum(tmp) / count)\n",
    "\n",
    "vectors_test = np.array(vectors_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer_test = KMeans(n_clusters=4)\n",
    "clusterer_test.fit(vectors)\n",
    "\n",
    "df_test[\"vectors\"] = list(vectors_test)\n",
    "df_test[\"cluster\"] = clusterer_test.predict(vectors_test)\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"transform_cluster\"] = [-1] * len(df_test[\"cluster\"])\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_value(cluster_value,transform_cluster, df):\n",
    "    for i in tqdm(range(len(df_test[\"cluster\"]))): \n",
    "        if df[\"cluster\"].values[i] == cluster_value: \n",
    "            df[\"transform_cluster\"].values[i]=transform_cluster\n",
    "\n",
    "replace_value(3,1,df_test)\n",
    "replace_value(2,2,df_test)\n",
    "replace_value(1,3,df_test)\n",
    "replace_value(0,0,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(df_test[\"label\"].values, df_test[\"transform_cluster\"].values))\n",
    "print(\"ACC:\", accuracy_score(df_test[\"label\"].values, df_test[\"transform_cluster\"].values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4650b2ab9af1b5111f53599d1b6662800e1e795f0a404b209a1518c877a28bdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
