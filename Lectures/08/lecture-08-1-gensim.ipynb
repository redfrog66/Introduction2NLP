{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Gensim = \"Generate Similar\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "   \"A survey of user opinion of computer system response time.\", \n",
    "   \"Relation of user perceived response time to error measurement.\", \n",
    "   \"The generation of random binary unordered trees.\", \n",
    "   \"The intersection graph of paths in trees.\", \n",
    "   \"Graph minors IV Widths of trees and well quasi ordering.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
       " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
       " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
       " ['intersection', 'graph', 'paths', 'trees'],\n",
       " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering']]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "processed_corpus = [[word for word in word_tokenize(document.lower()) \n",
    "                        if not word in stop_words and word.isalnum()] \n",
    "                    for document in corpus]\n",
    "\n",
    "processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dictionary(25 unique tokens: ['computer', 'opinion', 'response', 'survey', 'system']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'computer': 0,\n",
       " 'opinion': 1,\n",
       " 'response': 2,\n",
       " 'survey': 3,\n",
       " 'system': 4,\n",
       " 'time': 5,\n",
       " 'user': 6,\n",
       " 'error': 7,\n",
       " 'measurement': 8,\n",
       " 'perceived': 9,\n",
       " 'relation': 10,\n",
       " 'binary': 11,\n",
       " 'generation': 12,\n",
       " 'random': 13,\n",
       " 'trees': 14,\n",
       " 'unordered': 15,\n",
       " 'graph': 16,\n",
       " 'intersection': 17,\n",
       " 'paths': 18,\n",
       " 'iv': 19,\n",
       " 'minors': 20,\n",
       " 'ordering': 21,\n",
       " 'quasi': 22,\n",
       " 'well': 23,\n",
       " 'widths': 24}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(2, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
       " [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1)],\n",
       " [(14, 1), (16, 1), (17, 1), (18, 1)],\n",
       " [(14, 1), (16, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1)]]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# bag of word\n",
    "bow = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(14, 0.4869354917707381), (16, 0.8734379353188121)]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from gensim import models\n",
    "tfidf = models.TfidfModel(bow)\n",
    "words = \"trees graph\".lower().split()\n",
    "tfidf[dictionary.doc2bow(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['hello', 'how', 'are', 'you'],\n",
       " ['how', 'do', 'you', 'do'],\n",
       " ['hey',\n",
       "  'what',\n",
       "  'are',\n",
       "  'you',\n",
       "  'doing',\n",
       "  'yes',\n",
       "  'you',\n",
       "  'what',\n",
       "  'are',\n",
       "  'you',\n",
       "  'doing']]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "doc_list = [\n",
    "   \"Hello, how are you?\", \"How do you do?\", \n",
    "   \"Hey what are you doing? yes you What are you doing?\"\n",
    "]\n",
    "\n",
    "doc_tokenized = [simple_preprocess(doc) for doc in doc_list]\n",
    "doc_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(2, 1), (3, 1), (4, 2)],\n",
       " [(0, 2), (3, 3), (5, 2), (6, 1), (7, 2), (8, 1)]]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "doc_dictionary = corpora.Dictionary()\n",
    "doc_bow = [doc_dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
    "doc_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[('are', 1), ('hello', 1), ('how', 1), ('you', 1)], [('how', 1), ('you', 1), ('do', 2)], [('are', 2), ('you', 3), ('doing', 2), ('hey', 1), ('what', 2), ('yes', 1)]]\n"
     ]
    }
   ],
   "source": [
    "id_words = [[(doc_dictionary[id], count) for id, count in line] for line in doc_bow]\n",
    "print(id_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}