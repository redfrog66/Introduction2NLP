{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Minimal example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'brown': 1, 'dog': 1, 'fox': 1, 'jumps': 1, 'lazy': 1, 'over': 1, 'quick': 1, 'the': 2}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"the quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "VOCAB_SIZE = 10\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=VOCAB_SIZE)\n",
    "\n",
    "X = vectorizer.fit_transform([sentence]).toarray()\n",
    "\n",
    "print(dict(zip(vectorizer.get_feature_names(), X[0])))"
   ]
  },
  {
   "source": [
    "## Another example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id keyword        location  \\\n",
       "0   0  ablaze             NaN   \n",
       "1   1  ablaze             NaN   \n",
       "2   2  ablaze   New York City   \n",
       "3   3  ablaze  Morgantown, WV   \n",
       "4   4  ablaze             NaN   \n",
       "\n",
       "                                                text  target  \n",
       "0  Communal violence in Bhainsa, Telangana. \"Ston...       1  \n",
       "1  Telangana: Section 144 has been imposed in Bha...       1  \n",
       "2  Arsonist sets cars ablaze at dealership https:...       1  \n",
       "3  Arsonist sets cars ablaze at dealership https:...       1  \n",
       "4  \"Lord Jesus, your love brings freedom and pard...       0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>ablaze</td>\n      <td>NaN</td>\n      <td>Communal violence in Bhainsa, Telangana. \"Ston...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>ablaze</td>\n      <td>NaN</td>\n      <td>Telangana: Section 144 has been imposed in Bha...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>ablaze</td>\n      <td>New York City</td>\n      <td>Arsonist sets cars ablaze at dealership https:...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>ablaze</td>\n      <td>Morgantown, WV</td>\n      <td>Arsonist sets cars ablaze at dealership https:...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>ablaze</td>\n      <td>NaN</td>\n      <td>\"Lord Jesus, your love brings freedom and pard...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/tweets.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "## Extract relevant feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    Communal violence in Bhainsa, Telangana. \"Ston...\n",
       "1    Telangana: Section 144 has been imposed in Bha...\n",
       "2    Arsonist sets cars ablaze at dealership https:...\n",
       "3    Arsonist sets cars ablaze at dealership https:...\n",
       "4    \"Lord Jesus, your love brings freedom and pard...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "tweets = df[\"text\"]\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "source": [
    "## Remove stopwords"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\opell.DESKTOP-\n[nltk_data]     UEQ8DPV\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to C:\\Users\\opell.DESKTOP-\n[nltk_data]     UEQ8DPV\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before removing stopwords:\n0    Communal violence in Bhainsa, Telangana. \"Ston...\n1    Telangana: Section 144 has been imposed in Bha...\n2    Arsonist sets cars ablaze at dealership https:...\n3    Arsonist sets cars ablaze at dealership https:...\n4    \"Lord Jesus, your love brings freedom and pard...\nName: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Before removing stopwords:\")\n",
    "print(tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "tweets = tweets.apply(lambda row: \" \".join([word for word in word_tokenize(row) if not word in stop_words and word.isalpha()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "After removing stopwords:\n0    Communal violence Bhainsa Telangana Stones pel...\n1    Telangana Section imposed Bhainsa January clas...\n2           Arsonist sets cars ablaze dealership https\n3     Arsonist sets cars ablaze dealership https https\n4    Lord Jesus love brings freedom pardon Fill Hol...\nName: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"After removing stopwords:\")\n",
    "print(tweets.head())"
   ]
  },
  {
   "source": [
    "## Stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer  \n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before stemming:\n0    Communal violence Bhainsa Telangana Stones pel...\n1    Telangana Section imposed Bhainsa January clas...\n2           Arsonist sets cars ablaze dealership https\n3     Arsonist sets cars ablaze dealership https https\n4    Lord Jesus love brings freedom pardon Fill Hol...\nName: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Before stemming:\")\n",
    "print(tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.apply(stemmer.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "After stemming:\n0    communal violence bhainsa telangana stones pel...\n1    telangana section imposed bhainsa january clas...\n2            arsonist sets cars ablaze dealership http\n3      arsonist sets cars ablaze dealership https http\n4    lord jesus love brings freedom pardon fill hol...\nName: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"After stemming:\")\n",
    "print(tweets.head())"
   ]
  },
  {
   "source": [
    "## Bag-of-words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=VOCAB_SIZE)\n",
    "\n",
    "X = vectorizer.fit_transform(tweets).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(11370, 1000)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['able', 'absolutely', 'accident', 'account', 'across', 'act', 'action', 'activity', 'actually', 'affected', 'after', 'aftershock', 'ago', 'agree', 'air', 'airplane', 'alert', 'all', 'almost', 'alone', 'along', 'already', 'also', 'always', 'am', 'amazing', 'ambulance', 'america', 'american', 'amp', 'an', 'and', 'animal', 'animals', 'annihilated', 'another', 'anyone', 'anything', 'apocalypse', 'are', 'area', 'areas', 'army', 'around', 'arrested', 'arson', 'art', 'article', 'as', 'ash', 'ask', 'asked', 'asking', 'ass', 'at', 'attack', 'attacked', 'australia', 'australian', 'avalanche', 'away', 'baby', 'back', 'backup', 'bad', 'bag', 'bagging', 'bags', 'bang', 'base', 'batangas', 'battle', 'bbc', 'bc', 'be', 'beat', 'become', 'behind', 'believe', 'bernie', 'best', 'better', 'big', 'biggest', 'bill', 'billion', 'bioterror', 'bioterrorism', 'bit', 'black', 'blazing', 'bleeding', 'blew', 'blight', 'blizzard', 'blood', 'bloody', 'body', 'bomb', 'bombed', 'bomber', 'bombing', 'bombs', 'book', 'border', 'boy', 'brace', 'break', 'breaking', 'brendan', 'bridge', 'bring', 'british', 'bts', 'build', 'building', 'buildings', 'burn', 'burned', 'burning', 'bus', 'bush', 'bushfire', 'bushfires', 'business', 'but', 'buy', 'by', 'ca', 'call', 'called', 'calling', 'came', 'campaign', 'can', 'cant', 'car', 'care', 'case', 'casualties', 'casualty', 'cat', 'catastrophic', 'caught', 'cause', 'caused', 'causes', 'causing', 'central', 'change', 'check', 'chemical', 'chief', 'child', 'children', 'china', 'chinese', 'city', 'clear', 'clearly', 'cliff', 'climate', 'close', 'closed', 'coast', 'collapse', 'collapsed', 'collide', 'collided', 'collision', 'come', 'comes', 'coming', 'community', 'company', 'completely', 'continue', 'continues', 'control', 'cool', 'cost', 'could', 'countries', 'country', 'county', 'couple', 'course', 'court', 'crash', 'crashed', 'crazy', 'crisis', 'crush', 'crushed', 'curfew', 'current', 'currently', 'damage', 'damn', 'danger', 'data', 'day', 'days', 'dead', 'deal', 'dear', 'death', 'deaths', 'decided', 'deep', 'deluge', 'democrats', 'demolish', 'demolished', 'demolition', 'derail', 'derailed', 'derailment', 'desolate', 'destroy', 'destroyed', 'destruction', 'detonate', 'devastation', 'did', 'die', 'died', 'different', 'disaster', 'displaced', 'district', 'do', 'does', 'dog', 'don', 'done', 'dont', 'door', 'dr', 'dream', 'drive', 'driving', 'drop', 'drought', 'drown', 'drowned', 'drowning', 'dublin', 'due', 'dust', 'early', 'earth', 'earthquake', 'east', 'either', 'electrocute', 'else', 'emergency', 'end', 'english', 'engulfed', 'enough', 'entire', 'epicentre', 'eruption', 'est', 'evacuate', 'evacuation', 'even', 'event', 'ever', 'every', 'everyone', 'everything', 'evil', 'exactly', 'experienced', 'explosion', 'eyes', 'eyewitness', 'face', 'facing', 'fact', 'failure', 'fake', 'fall', 'families', 'family', 'fan', 'fans', 'far', 'fatal', 'fatalities', 'fatality', 'father', 'fear', 'feel', 'feeling', 'felt', 'fight', 'finally', 'find', 'fir', 'fire', 'fires', 'first', 'five', 'flames', 'flattened', 'flight', 'flood', 'flooding', 'floods', 'follow', 'following', 'food', 'footage', 'for', 'force', 'forced', 'forest', 'forget', 'found', 'four', 'free', 'freight', 'friend', 'friends', 'from', 'front', 'fuck', 'fucking', 'full', 'future', 'game', 'gave', 'get', 'gets', 'getting', 'girl', 'give', 'given', 'giving', 'global', 'go', 'god', 'goes', 'going', 'gon', 'gone', 'good', 'got', 'gove', 'government', 'govt', 'great', 'ground', 'group', 'gt', 'guess', 'gun', 'guy', 'guys', 'hail', 'half', 'hand', 'happen', 'happened', 'happy', 'hard', 'harm', 'has', 'hate', 'have', 'hazard', 'hazardous', 'he', 'head', 'health', 'hear', 'heard', 'heart', 'heat', 'heavy', 'held', 'helicopter', 'hell', 'hellfire', 'help', 'her', 'here', 'hey', 'hi', 'high', 'hijack', 'hijacker', 'hijacking', 'his', 'history', 'hit', 'hitchin', 'hold', 'home', 'homes', 'hope', 'hospital', 'hostage', 'hostages', 'hour', 'hours', 'house', 'houses', 'how', 'http', 'https', 'huge', 'human', 'hurricane', 'hurt', 'id', 'idea', 'if', 'im', 'imagine', 'impact', 'important', 'in', 'incident', 'including', 'india', 'indian', 'info', 'information', 'injured', 'injuries', 'injury', 'innocent', 'inside', 'instead', 'internet', 'interview', 'inundated', 'inundation', 'iran', 'iranian', 'iraq', 'ireland', 'is', 'isis', 'islamic', 'island', 'israel', 'issues', 'it', 'its', 'jan', 'january', 'japan', 'jesus', 'job', 'joe', 'john', 'join', 'just', 'justice', 'kashmir', 'keep', 'kids', 'kill', 'killed', 'killing', 'kind', 'king', 'know', 'land', 'landslide', 'lane', 'large', 'last', 'late', 'latest', 'law', 'lead', 'least', 'leave', 'leaving', 'left', 'less', 'let', 'level', 'life', 'light', 'lightning', 'lik', 'like', 'likely', 'line', 'link', 'list', 'listen', 'literally', 'little', 'live', 'lives', 'living', 'local', 'lol', 'london', 'long', 'longer', 'look', 'looking', 'looks', 'lost', 'lot', 'loud', 'love', 'low', 'lvl', 'made', 'major', 'make', 'makes', 'making', 'man', 'many', 'market', 'mass', 'massive', 'matter', 'may', 'maybe', 'me', 'mean', 'means', 'media', 'melbourne', 'melissa', 'meltdown', 'members', 'men', 'middle', 'might', 'military', 'million', 'millions', 'mind', 'minister', 'minutes', 'miss', 'missile', 'missing', 'mom', 'moment', 'monday', 'money', 'months', 'more', 'morning', 'mother', 'move', 'movie', 'moving', 'much', 'mudslide', 'multiple', 'murder', 'music', 'muslim', 'must', 'my', 'mystery', 'na', 'name', 'nation', 'national', 'natural', 'nc', 'ne', 'near', 'nearly', 'need', 'needed', 'needs', 'never', 'new', 'news', 'next', 'nice', 'night', 'no', 'normal', 'north', 'not', 'nothing', 'now', 'nuclear', 'number', 'obama', 'obliterate', 'obliterated', 'of', 'official', 'officials', 'oh', 'oil', 'ok', 'old', 'on', 'one', 'only', 'open', 'or', 'order', 'others', 'our', 'out', 'outbreak', 'outside', 'over', 'pakistan', 'panic', 'panicking', 'parents', 'park', 'part', 'parts', 'party', 'passengers', 'past', 'pay', 'peopl', 'people', 'person', 'philippines', 'phone', 'pick', 'picture', 'piece', 'place', 'plan', 'plane', 'planet', 'plant', 'play', 'please', 'pm', 'point', 'police', 'political', 'poor', 'possible', 'post', 'power', 'powerful', 'pray', 'president', 'pretty', 'probably', 'problem', 'protect', 'protests', 'public', 'puerto', 'push', 'put', 'quality', 'quarantine', 'quarantined', 'question', 'rain', 'razed', 'reactor', 'read', 'ready', 'real', 'really', 'reason', 'record', 'red', 'refugees', 'regime', 'region', 'relief', 'remember', 'report', 'reported', 'rescue', 'rescued', 'rescuers', 'research', 'residents', 'responders', 'response', 'result', 'return', 'rico', 'right', 'riot', 'rioting', 'risk', 'river', 'rn', 'road', 'rock', 'role', 'room', 'royal', 'rt', 'ruin', 'run', 'running', 'sad', 'safe', 'safety', 'said', 'sandstorm', 'saudi', 'save', 'saved', 'saw', 'say', 'saying', 'says', 'scared', 'school', 'schools', 'screamed', 'screaming', 'screams', 'sea', 'season', 'second', 'security', 'see', 'seeing', 'seems', 'seen', 'seismic', 'send', 'sense', 'sent', 'series', 'serious', 'service', 'services', 'set', 'several', 'severe', 'share', 'she', 'shit', 'shooting', 'short', 'shot', 'show', 'shows', 'shut', 'sick', 'side', 'since', 'singh', 'single', 'sinkhole', 'sinking', 'sir', 'sirens', 'situation', 'six', 'sleep', 'sleeping', 'small', 'smoke', 'snow', 'snowstorm', 'so', 'soldiers', 'soleimani', 'some', 'someone', 'something', 'sometimes', 'son', 'song', 'soon', 'sorry', 'soul', 'sounds', 'south', 'speak', 'speed', 'spill', 'st', 'stand', 'star', 'start', 'started', 'state', 'states', 'station', 'stay', 'still', 'stop', 'storm', 'stormbrendan', 'story', 'stream', 'street', 'stretcher', 'strong', 'students', 'stupid', 'suddenly', 'suicide', 'sun', 'sunday', 'sunk', 'super', 'support', 'sure', 'survive', 'survived', 'survivors', 'swallows', 'system', 'taal', 'take', 'taken', 'takes', 'taking', 'talk', 'talking', 'team', 'tell', 'telling', 'terrorism', 'terrorist', 'terrorists', 'th', 'thailand', 'thank', 'that', 'the', 'then', 'there', 'these', 'they', 'thing', 'things', 'think', 'thinking', 'this', 'though', 'thought', 'thousands', 'thread', 'threat', 'three', 'thunder', 'thunderstorm', 'till', 'time', 'times', 'to', 'today', 'together', 'told', 'tonight', 'took', 'top', 'tornado', 'total', 'totally', 'tour', 'towards', 'town', 'traffic', 'tragedy', 'train', 'training', 'trapped', 'trauma', 'traumatised', 'tried', 'triumphant', 'trouble', 'truck', 'true', 'trump', 'trust', 'try', 'trying', 'tuesday', 'turn', 'tweet', 'twitter', 'two', 'uk', 'ukrainian', 'understand', 'united', 'unless', 'up', 'update', 'upheaval', 'ur', 'us', 'usa', 'use', 'used', 'using', 'vehicle', 'very', 'via', 'victims', 'video', 'violence', 'violent', 'virus', 'visit', 'volcanic', 'volcano', 'vote', 'vs', 'wait', 'waiting', 'wake', 'walk', 'wan', 'want', 'wanted', 'wants', 'war', 'warning', 'warren', 'was', 'watch', 'watching', 'water', 'wave', 'way', 'we', 'weapon', 'weapons', 'weather', 'week', 'weekend', 'weeks', 'weird', 'well', 'went', 'west', 'western', 'what', 'whatever', 'when', 'where', 'whether', 'white', 'who', 'whole', 'why', 'wild', 'wildfire', 'wildlife', 'will', 'win', 'wind', 'winds', 'windstorm', 'wish', 'with', 'within', 'without', 'wo', 'woman', 'women', 'wonder', 'word', 'work', 'working', 'world', 'worse', 'worst', 'would', 'wounded', 'wounds', 'wreck', 'wrecked', 'wrong', 'yeah', 'year', 'years', 'yes', 'yesterday', 'yet', 'you', 'young', 'your', 'zone']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   able  absolutely  accident  account  across  act  action  activity  \\\n",
       "0     0           0         0        0       0    0       0         0   \n",
       "1     0           0         0        0       0    0       0         0   \n",
       "2     0           0         0        0       0    0       0         0   \n",
       "3     0           0         0        0       0    0       0         0   \n",
       "4     0           0         0        0       0    0       0         0   \n",
       "\n",
       "   actually  affected  ...  yeah  year  years  yes  yesterday  yet  you  \\\n",
       "0         0         0  ...     0     0      0    0          0    0    0   \n",
       "1         0         0  ...     0     0      0    0          0    0    0   \n",
       "2         0         0  ...     0     0      0    0          0    0    0   \n",
       "3         0         0  ...     0     0      0    0          0    0    0   \n",
       "4         0         0  ...     0     0      0    0          0    0    0   \n",
       "\n",
       "   young  your  zone  \n",
       "0      0     0     0  \n",
       "1      0     0     0  \n",
       "2      0     0     0  \n",
       "3      0     0     0  \n",
       "4      0     0     0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>able</th>\n      <th>absolutely</th>\n      <th>accident</th>\n      <th>account</th>\n      <th>across</th>\n      <th>act</th>\n      <th>action</th>\n      <th>activity</th>\n      <th>actually</th>\n      <th>affected</th>\n      <th>...</th>\n      <th>yeah</th>\n      <th>year</th>\n      <th>years</th>\n      <th>yes</th>\n      <th>yesterday</th>\n      <th>yet</th>\n      <th>you</th>\n      <th>young</th>\n      <th>your</th>\n      <th>zone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1000 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "vectorized = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "\n",
    "vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "type(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     able  absolutely  accident  account  across  act  action  activity  \\\n",
       "135     0           0         2        0       0    0       0         0   \n",
       "\n",
       "     actually  affected  ...  yeah  year  years  yes  yesterday  yet  you  \\\n",
       "135         0         0  ...     0     0      0    0          0    0    0   \n",
       "\n",
       "     young  your  zone  \n",
       "135      0     0     0  \n",
       "\n",
       "[1 rows x 1000 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>able</th>\n      <th>absolutely</th>\n      <th>accident</th>\n      <th>account</th>\n      <th>across</th>\n      <th>act</th>\n      <th>action</th>\n      <th>activity</th>\n      <th>actually</th>\n      <th>affected</th>\n      <th>...</th>\n      <th>yeah</th>\n      <th>year</th>\n      <th>years</th>\n      <th>yes</th>\n      <th>yesterday</th>\n      <th>yet</th>\n      <th>you</th>\n      <th>young</th>\n      <th>your</th>\n      <th>zone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>135</th>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 1000 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "accident = vectorized.loc[vectorized['accident'] > 1]\n",
    "accident.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}