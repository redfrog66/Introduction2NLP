{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('E01': conda)",
   "metadata": {
    "interpreter": {
     "hash": "71f835d09f2dc7658b8bfea70337fd38a098aa1e53e53fe645aaa17deb87e84b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Character tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "i would like to work than machine lerning engineer at google!\niwouldliketoworkthanmachinelerningengineeratgoogle!\n['i', 'w', 'o', 'u', 'l', 'd', 'l', 'i', 'k', 'e', 't', 'o', 'w', 'o', 'r', 'k', 't', 'h', 'a', 'n', 'm', 'a', 'c', 'h', 'i', 'n', 'e', 'l', 'e', 'r', 'n', 'i', 'n', 'g', 'e', 'n', 'g', 'i', 'n', 'e', 'e', 'r', 'a', 't', 'g', 'o', 'o', 'g', 'l', 'e', '!']\n{'l', 't', 'c', 'g', 'o', 'w', 'a', 'u', 'i', 'h', '!', 'n', 'r', 'e', 'k', 'd', 'm'}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I would like to work than machine lerning engineer at Google!\".lower()\n",
    "print(sentence)\n",
    "\n",
    "sentence = sentence.replace(\" \",\"\")\n",
    "print(sentence)\n",
    "\n",
    "chars = [char for char in sentence]\n",
    "print(chars)\n",
    "\n",
    "chars = set(chars)\n",
    "print(chars)"
   ]
  },
  {
   "source": [
    "## Word tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    'i love my dog',\n",
    "    'I, love my cat',\n",
    "    'You love my dog!'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['i', 'love', 'my', 'dog']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokens = word_tokenize(sentences[0])\n",
    "\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/tokenizers/python/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<tokenizers.Tokenizer object at 0x0000022C59AFE7C0>\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<tokenizers.trainers.BpeTrainer object at 0x0000022C59665110>\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "print(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f\"data/lecture-03/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"data/lecture-03/tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"data/lecture-03/tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Encoding(num_tokens=11, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(\"Hello, y'all! How are you 游때 ?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Hello', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '[UNK]', '?']\n[27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]\n(26, 27)\n"
     ]
    }
   ],
   "source": [
    "print(output.tokens)\n",
    "print(output.ids)\n",
    "print(output.offsets[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "tokenizer.token_to_id(\"[SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Hello', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '[UNK]', '?']\n",
      "['[CLS]', 'Hello', ',', 'y', \"'\", 'all', '!', '[SEP]', 'How', 'are', 'you', '[UNK]', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(output.tokens)\n",
    "output = tokenizer.encode(\"Hello, y'all!\", \"How are you 游때 ?\")\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(output.type_ids)"
   ]
  },
  {
   "source": [
    "## Encoding multiple sentences in a batch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you 游때 ?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode_batch(\n",
    "    [[\"Hello, y'all!\", \"How are you 游때 ?\"], [\"Hello to you too!\", \"I'm fine, thank you!\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.enable_padding(pad_id=3, pad_token=\"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['[CLS]', 'How', 'are', 'you', '[UNK]', '?', '[SEP]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you 游때 ?\"])\n",
    "print(output[1].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(output[1].attention_mask)"
   ]
  },
  {
   "source": [
    "## Most used tokenizers (We will learn all of them in advanced NLP)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.models import BPE\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.models import WordPiece"
   ]
  },
  {
   "source": [
    "## Using a pretrained tokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\"data/lecture-03/vocabs/bert-base-uncased-vocab.txt\", lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['[CLS]', 'hello', ',', 'y', \"'\", 'all', '!', '[SEP]', 'how', 'are', 'you', '[UNK]', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(\"Hello, y'all!\", \"How are you 游때 ?\")\n",
    "print(output.tokens)"
   ]
  }
 ]
}