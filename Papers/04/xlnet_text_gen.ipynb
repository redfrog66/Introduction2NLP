{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "228ef76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80aa1989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "906eb33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dee348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
    "import torch\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
    "model = XLNetLMHeadModel.from_pretrained('xlnet-large-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "568e9cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "(except for Alexei and Maria) are discovered.\n",
    "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "remainder of the story. 1883 Western Siberia,\n",
    "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "with people, even a bishop, begging for his blessing.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf0340ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def choose_from_top(probs, k=5, sample_size=1):\n",
    "    ind = np.argpartition(probs, -k)[-k:]\n",
    "    top_prob = probs[ind]\n",
    "    top_prob = top_prob / np.sum(top_prob)\n",
    "    choice = np.random.choice(k, sample_size, p = top_prob, replace=False)\n",
    "    token_ids = ind[choice]\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4007ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"If sentence is not as long as the desired sentence length,\"\n",
    "topk = 10\n",
    "n = 20\n",
    "temperature = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dc80708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183, 3833, 27, 50, 34, 206, 34, 18, 7181, 3833, 2486, 19]\n",
      "[6]\n",
      "[67, 2840, 19, 18, 1484, 20, 965, 29077, 8719, 1273, 21, 45, 273, 17, 10, 15048, 28, 27511, 21, 4185, 11, 41, 2444, 9, 32, 1025, 20, 8719, 26, 23, 673, 966, 19, 29077, 20643, 27511, 20822, 20643, 19, 17, 6616, 17511, 18, 8978, 20, 18, 777, 9, 19233, 1527, 17669, 19, 24, 673, 17, 28756, 150, 12943, 4354, 153, 27, 442, 37, 45, 668, 21, 24, 256, 20, 416, 22, 2771, 4901, 9, 12943, 4354, 153, 51, 24, 3004, 21, 28142, 23, 65, 20, 18, 416, 34, 24, 2958, 22947, 9, 1177, 45, 668, 3097, 13768, 23, 103, 28, 441, 148, 48, 20522, 19, 12943, 4354, 153, 12860, 34, 18, 326, 27, 17492, 684, 21, 6709, 9, 8585, 123, 266, 19, 12943, 4354, 153, 6872, 24, 3004, 20, 18, 9225, 2198, 19, 12717, 103, 22, 401, 24, 6348, 9, 12943, 4354, 153, 1068, 2768, 2286, 19, 33, 104, 19, 176, 24, 9313, 19, 20086, 28, 45, 10292, 9]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "if torch.cuda.is_available(): model.to('cuda')\n",
    "\n",
    "sent_tokens = tokenizer.encode(sent, add_special_tokens=False)\n",
    "print(sent_tokens)\n",
    "mask_tokens = tokenizer.encode('<mask>', add_special_tokens=False)\n",
    "print(mask_tokens)\n",
    "padding_tokens = tokenizer.encode(PADDING_TEXT, add_special_tokens=False)\n",
    "print(padding_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85155161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If sentence is not as long as the desired sentence length, or in addition to the sentence length, then the sentence is not considered a sentence. A sentence is.. and....s.....s when. on...\n"
     ]
    }
   ],
   "source": [
    "for i in range(n):\n",
    "    input = sent_tokens + mask_tokens\n",
    "    target_id1 = -len(input)\n",
    "    target_id2 = -1\n",
    "\n",
    "    input_ids = torch.tensor(padding_tokens).unsqueeze(0)\n",
    "    perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
    "    perm_mask[0, :, [target_id1, target_id2]] = 1.0\n",
    "\n",
    "    target_mapping = torch.zeros((1, 2, input_ids.shape[1]), dtype=torch.float) \n",
    "    target_mapping[0, 0, target_id1] = 1.0\n",
    "    target_mapping[0, 1, target_id2] = 1.0\n",
    "    #print(target_mapping)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids_tensor = input_ids.to(\"cuda\")\n",
    "        target_mapping_tensor = target_mapping.to(\"cuda\")\n",
    "        perm_mask_tensor = perm_mask.to(\"cuda\")\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "        target_mapping_tensor = target_mapping\n",
    "        perm_mask_tensor = perm_mask\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids_tensor, perm_mask=perm_mask_tensor, target_mapping=target_mapping_tensor)\n",
    "\n",
    "    predicted_tokens = []\n",
    "    \n",
    "    for j in range(2):\n",
    "        probs = torch.nn.functional.softmax(outputs[0][0][j]/temperature, dim = 0).to('cpu').numpy()\n",
    "        predicted_tokens.append(choose_from_top(probs, k=topk, sample_size=4))\n",
    "    tok = predicted_tokens[1][0]\n",
    "    sent_tokens = sent_tokens + [tok]\n",
    "    \n",
    "print(tokenizer.decode(sent_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e9dede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c0bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
